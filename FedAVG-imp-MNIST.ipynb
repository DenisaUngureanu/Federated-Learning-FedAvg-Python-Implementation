{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import random\n",
    "import cv2 \n",
    "import os\n",
    "from imutils import paths\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import accuracy_score\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers.legacy import SGD\n",
    "from tensorflow.keras import backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(paths, verbose=-1):\n",
    "    '''expects images for each class in seperate dir, \n",
    "    e.g all digits in 0 class in the directory named 0 '''\n",
    "    data = list()\n",
    "    labels = list()\n",
    "    # loop over the input images\n",
    "    for (i, imgpath) in enumerate(paths):\n",
    "        # load the image and extract the class labels\n",
    "        im_gray = cv2.imread(imgpath, cv2.IMREAD_GRAYSCALE) # the image is read as a gray scale\n",
    "        image = np.array(im_gray).flatten() # the image is flattened #Return a copy of the array collapsed into one dimension.\n",
    "        label = imgpath.split(os.path.sep)[-2] # for obtain the class label \n",
    "        # scale the image to [0, 1] and add to list\n",
    "        data.append(image/255) # scale the image to [0, 1]\n",
    "        labels.append(label)\n",
    "        # show an update every `verbose` images\n",
    "        if verbose > 0 and i > 0 and (i + 1) % verbose == 0:\n",
    "            print(\"[INFO] processed {}/{}\".format(i + 1, len(paths)))\n",
    "    # return a tuple of the data and labels\n",
    "    return data, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#declare path to your mnist data folder\n",
    "img_path = ' '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the path list using the path object\n",
    "image_paths = list(paths.list_images(img_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] processed 10000/42000\n",
      "[INFO] processed 20000/42000\n",
      "[INFO] processed 30000/42000\n",
      "[INFO] processed 40000/42000\n"
     ]
    }
   ],
   "source": [
    "#apply our function\n",
    "image_list, label_list = load(image_paths, verbose=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converteste etichetele claselor Ã®n format binar.\n",
    "lb = LabelBinarizer()\n",
    "label_list = lb.fit_transform(label_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split data into training and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(image_list, \n",
    "                                                    label_list, \n",
    "                                                    test_size=0.1, \n",
    "                                                    random_state=42)\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "X_test = np.array(X_test)\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_clientsIID(image_list, label_list, num_clients=1, initial='clients'):\n",
    "    '''\n",
    "        args: \n",
    "            image_list: a list of numpy arrays of training images\n",
    "            label_list:a list of binarized labels for each image\n",
    "            num_client: number of federated members (clients)\n",
    "            initials: the clients'name prefix, e.g, clients_1       \n",
    "    '''\n",
    "    #create a list of client names\n",
    "    client_names = ['{}_{}'.format(initial, i+1)\n",
    "                    for i in range(num_clients)]\n",
    "    #randomize the data\n",
    "    data = list(zip(image_list, label_list)) #list of tuples\n",
    "    random.seed(10) #get the same random results\n",
    "    random.shuffle(data)\n",
    "\n",
    "    #shard data and place at each client\n",
    "    size = len(data)//num_clients\n",
    "    shards = [data[i:i + size] for i in range(0, size*num_clients, size)]\n",
    "    #number of clients must equal number of shards\n",
    "    assert(len(shards) == len(client_names))\n",
    "    return {client_names[i] : shards[i] for i in range(len(client_names))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " client_1 : 3780 , [0 0 0 0 1 0 0 0 0 0].\n",
      " client_2 : 3780 , [0 0 0 0 1 0 0 0 0 0].\n",
      " client_3 : 3780 , [0 0 1 0 0 0 0 0 0 0].\n",
      " client_4 : 3780 , [0 0 0 0 1 0 0 0 0 0].\n",
      " client_5 : 3780 , [0 0 0 0 0 0 1 0 0 0].\n",
      " client_6 : 3780 , [0 0 1 0 0 0 0 0 0 0].\n",
      " client_7 : 3780 , [0 1 0 0 0 0 0 0 0 0].\n",
      " client_8 : 3780 , [0 0 1 0 0 0 0 0 0 0].\n",
      " client_9 : 3780 , [0 0 0 0 1 0 0 0 0 0].\n",
      " client_10 : 3780 , [0 0 1 0 0 0 0 0 0 0].\n"
     ]
    }
   ],
   "source": [
    "clients= create_clientsIID(X_train, y_train, num_clients=10, initial='client')\n",
    "\n",
    "for client_name, client_data in clients.items():\n",
    "    num_images = len(client_data)  \n",
    "    client_label = client_data[3][1] \n",
    "    print(f\" {client_name} : {num_images} , {client_label}.\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flip_labels(clients, flip_fraction, clients_to_flip):\n",
    "    '''\n",
    "    flip a fraction of labels for selected clients \n",
    "      clients (dict): Dictionary containing client data \n",
    "      flip_fraction (float): Fraction of labels to flip\n",
    "      clients_to_flip (list): List of client names for which labels will be flipped.\n",
    "    '''\n",
    "    for client_name in clients_to_flip:\n",
    "        data = clients[client_name]  \n",
    "        # Calculate the number of labels to flip for this client\n",
    "        flipped_count = int(len(data) * flip_fraction) \n",
    "        # Randomly select indices of the labels to be flipped\n",
    "        flip_indices = random.sample(range(len(data)), flipped_count)  \n",
    "        \n",
    "        # Loop over each selected index to flip the corresponding label\n",
    "        for i in flip_indices:\n",
    "            original_label = data[i][1] \n",
    "            new_label = random.randint(0, 9)  \n",
    "            while new_label == np.argmax(original_label):\n",
    "                new_label = random.randint(0, 9)\n",
    "\n",
    "            flipped_label = np.zeros(original_label.shape)\n",
    "            flipped_label[new_label] = 1\n",
    "            data[i] = (data[i][0], flipped_label)\n",
    "\n",
    "    return clients\n",
    "\n",
    "clients_to_flip = random.sample(list(clients.keys()), 8)\n",
    "flipped_clients= flip_labels(clients, 1, clients_to_flip)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_data(data_shard, bs=32):\n",
    "    '''Takes in a clients data shard and create a tfds object off it\n",
    "    args:\n",
    "        shard: a data, label constituting a client's data shard\n",
    "        bs:batch size\n",
    "    return:\n",
    "        tfds object'''\n",
    "    #separate shard into data and labels lists\n",
    "    data, label = zip(*data_shard)\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((list(data), list(label)))\n",
    "    return dataset.shuffle(len(label)).batch(bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#process and batch the training data for each client\n",
    "clients_batched = dict()\n",
    "for (client_name, data) in clients.items():\n",
    "    clients_batched[client_name] = batch_data(data)\n",
    "\n",
    "#process and batch the test set  \n",
    "test_batched = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleMLP:\n",
    "    @staticmethod\n",
    "    def build(shape, classes):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(256, input_shape=(shape,)))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(Dense(128))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(Dense(classes))\n",
    "        model.add(Activation(\"softmax\"))\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "comms_round = 100\n",
    "\n",
    "loss='categorical_crossentropy'\n",
    "metrics = ['accuracy']\n",
    "\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=0.01,\n",
    "    decay_steps=10000, \n",
    "    decay_rate=0.9) \n",
    "\n",
    "# Create the optimizer with the learning rate schedule\n",
    "optimizer = SGD(learning_rate=lr_schedule, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#global_count = total number of data used for all clients\n",
    "#local_count = number of data for one client\n",
    "def weight_scalling_factor(clients_trn_data, client_name):\n",
    "    client_names = list(clients_trn_data.keys())\n",
    "    #get the bs\n",
    "    bs = list(clients_trn_data[client_name])[0][0].shape[0]\n",
    "    #first calculate the total training data points across clients\n",
    "    global_count = sum([tf.data.experimental.cardinality(\n",
    "                   clients_trn_data[client_name]).numpy() \n",
    "                   for client_name in client_names])*bs\n",
    "    # get the total number of data points held by a client\n",
    "    local_count = tf.data.experimental.cardinality(\n",
    "                  clients_trn_data[client_name]).numpy()*bs\n",
    "    return local_count/global_count\n",
    "\n",
    "def scale_model_weights(weight, scalar):\n",
    "    '''function for scaling a models weights'''\n",
    "    weight_final = []\n",
    "    steps = len(weight)\n",
    "    for i in range(steps):\n",
    "        weight_final.append(scalar * weight[i])\n",
    "    return weight_final\n",
    "\n",
    "\n",
    "def sum_scaled_weights(scaled_weight_list):\n",
    "    '''Return the sum of the listed scaled weights.\n",
    "    The is equivalent to scaled avg of the weights'''\n",
    "    avg_grad = list()\n",
    "    #get the average grad accross all client gradients\n",
    "    for grad_list_tuple in zip(*scaled_weight_list):\n",
    "        layer_mean = tf.math.reduce_sum(grad_list_tuple, axis=0)\n",
    "        avg_grad.append(layer_mean)\n",
    "    return avg_grad\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(X_test, Y_test,  model, comm_round):\n",
    "    global performanceExport\n",
    "    \n",
    "    cce = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "    logits = model.predict(X_test)\n",
    "    loss = cce(Y_test, logits)\n",
    "    acc = accuracy_score(tf.argmax(logits, axis=1), tf.argmax(Y_test, axis=1))\n",
    "    \n",
    "    new_performanceExport = pd.DataFrame({'Model':'GLOBAL', 'Com_Round': int(comm_round), 'accuracy': float(acc * 100), 'loss': float(loss)}, index=[0])\n",
    "    performanceExport = pd.concat([performanceExport, new_performanceExport])\n",
    "    \n",
    "    path = f' '\n",
    "    performanceExport.to_excel (path, index = False, header=True)\n",
    "   \n",
    "    print('comm_round: {} | global_acc: {:.3%} | global_loss: {}'.format(comm_round, acc, loss))\n",
    "    return acc, loss\n",
    "\n",
    "#print the performances from each client using test data\n",
    "def printLocalPerfm(X_test, y_test, model, comm_round, clientID):\n",
    "    global performanceExport\n",
    "    global accuracy\n",
    "    \n",
    "    #list for local accuracy and loss (for each model from client)\n",
    "    global local_acc_list \n",
    "    local_acc_list = []\n",
    "    global local_loss_list\n",
    "    local_loss_list = []\n",
    "    \n",
    "    cce = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "    logits = model.predict(X_test) \n",
    "    loss = cce(Y_test, logits) \n",
    "    acc = accuracy_score(tf.argmax(logits, axis=1), tf.argmax(Y_test, axis=1))\n",
    "    local_acc_list.append(loss)\n",
    "    local_loss_list.append(acc)\n",
    "\n",
    "    new_performanceExport = pd.DataFrame({'Model': f'model_{int(clientID)}', 'Com_Round': int(comm_round), 'accuracy': float(acc * 100), 'loss': float(loss)}, index=[0])\n",
    "    performanceExport = pd.concat([performanceExport, new_performanceExport])\n",
    "   \n",
    "    path = f' '\n",
    "    performanceExport.to_excel (path, index = False, header=True) \n",
    "    \n",
    "    print('comm_round: {} | local_acc: {:.3%} | local_loss: {}'.format(comm_round, acc, loss))\n",
    "    return acc, loss\n",
    "\n",
    "\n",
    "performanceExport = pd.DataFrame(columns = ['Model', 'Com_Round', 'accuracy', 'loss'])\n",
    "accuracy = pd.DataFrame(columns = ['Model', 'Com_Round', 'accuracy'])\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Metoda 1 \n",
    "start_time = time.time()\n",
    "smlp_global = SimpleMLP()\n",
    "global_model = smlp_global.build(784, 10)\n",
    "\n",
    "#commence global training loop\n",
    "for comm_round in range(comms_round):        \n",
    "    # get the global model's weights\n",
    "    global_weights = global_model.get_weights()\n",
    "    #initial list to collect local model weights after scalling\n",
    "    scaled_local_weight_list = list()\n",
    "    client_names= list(clients_batched.keys())\n",
    "    random.seed(448)\n",
    "    random.shuffle(client_names)\n",
    "    clientID = 0\n",
    "    client_performances = []\n",
    "    num_rejected_clients=0\n",
    "    num_accepted_clients=0\n",
    "    \n",
    "    #loop through each client and create new local model\n",
    "    for client in client_names:\n",
    "        smlp_local = SimpleMLP()\n",
    "        local_model = smlp_local.build(784,10)\n",
    "        local_model.compile(loss=loss, \n",
    "                      optimizer=optimizer, \n",
    "                      metrics=metrics)\n",
    "        \n",
    "        local_model.set_weights(global_weights)\n",
    "\n",
    "        local_model.fit(clients_batched[client], epochs=1, verbose=0)\n",
    "\n",
    "        for(X_test, Y_test) in test_batched:\n",
    "            local_acc, local_loss = printLocalPerfm(X_test, Y_test, local_model,\n",
    "                                                    comm_round, clientID)\n",
    "        #check clients to accept or reject\n",
    "        if comm_round == 0 or local_acc >= global_acc:\n",
    "            num_accepted_clients+=1\n",
    "            scaling_factor = 1\n",
    "            print('accepted')\n",
    "        else:\n",
    "            print('rejected')\n",
    "            scaling_factor = 0 \n",
    "            num_rejected_clients+=1\n",
    "        \n",
    "        client_performances.append((client, local_acc, scaling_factor,\n",
    "                                    local_model.get_weights()))\n",
    "        \n",
    "        #clear session to free memory after each communication round\n",
    "        K.clear_session()\n",
    "        clientID += 1\n",
    "        \n",
    "    #determine the scaling factor based on number of accepted clients\n",
    "    if num_accepted_clients > 0:\n",
    "        final_scaling_factor = 1.0 / num_accepted_clients\n",
    "    else:\n",
    "        final_scaling_factor = 0\n",
    "        \n",
    "    if num_rejected_clients==10:\n",
    "        print('All clients were rejected')\n",
    "        # sort client performances and select top 5 clients\n",
    "        client_performances.sort(key=lambda x: x[1], reverse=True)\n",
    "        top_5_clients = client_performances[:5]\n",
    "        for client_info in  top_5_clients:\n",
    "            scaling_factor = 0.2\n",
    "            scaled_weights = scale_model_weights(client_info[3], scaling_factor)\n",
    "            scaled_local_weight_list.append(scaled_weights) \n",
    "    else:\n",
    "        for client_info in client_performances:\n",
    "            _, _, scaling_factor, weights = client_info\n",
    "            if scaling_factor > 0:\n",
    "                scaling_factor = final_scaling_factor\n",
    "            scaled_weights = scale_model_weights(weights, scaling_factor)\n",
    "            scaled_local_weight_list.append(scaled_weights)\n",
    "            print(scaling_factor)\n",
    "    average_weights = sum_scaled_weights(scaled_local_weight_list)\n",
    "        \n",
    "    #update global model \n",
    "    global_model.set_weights(average_weights)\n",
    "    for(X_test, Y_test) in test_batched:\n",
    "        global_acc, global_loss = test_model(X_test, Y_test, global_model, comm_round)\n",
    "\n",
    "end_time = time.time()\n",
    "training_time = end_time - start_time\n",
    "print(\"Timpul de antrenare:\", training_time, \"secunde\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Metoda 2\n",
    "#initialize global model\n",
    "start_time = time.time()\n",
    "smlp_global = SimpleMLP()\n",
    "global_model = smlp_global.build(784, 10) \n",
    "#commence global training loop\n",
    "for comm_round in range(comms_round):        \n",
    "    # get the global model's weights \n",
    "    global_weights = global_model.get_weights()\n",
    "    \n",
    "    #initial list to collect local model weights after scalling\n",
    "    scaled_local_weight_list = list()\n",
    "    clients_performances = []\n",
    "\n",
    "    client_names= list(clients_batched.keys())\n",
    "    random.seed(448)\n",
    "    random.shuffle(client_names)\n",
    "    clientID = 0\n",
    "    #loop through each client and create new local model\n",
    "    for client in client_names:\n",
    "        smlp_local = SimpleMLP()\n",
    "        local_model = smlp_local.build(784, 10)\n",
    "        local_model.compile(loss=loss, \n",
    "                      optimizer=optimizer, \n",
    "                      metrics=metrics)\n",
    "        \n",
    "        #set local model weight to the weight of the global model\n",
    "        local_model.set_weights(global_weights)\n",
    "     \n",
    "        local_model.fit(clients_batched[client], epochs=1, verbose=0)\n",
    "        if comm_round == 0:  \n",
    "            scaling_factor = 0.1\n",
    "        else:\n",
    "            scaling_factor = 0.2   \n",
    "    \n",
    "        for(X_test, Y_test) in test_batched:\n",
    "            local_acc, local_loss = printLocalPerfm(X_test, Y_test, \n",
    "                                    local_model, comm_round, clientID)\n",
    "        clients_performances.append((client, local_acc, \n",
    "                                     scaling_factor, local_model.get_weights()))\n",
    "        K.clear_session()\n",
    "        clientID += 1\n",
    "           \n",
    "    if comm_round == 0:   \n",
    "        for client_info in clients_performances:\n",
    "            scaled_weights = scale_model_weights(client_info[3], client_info[2])\n",
    "            scaled_local_weight_list.append( scaled_weights)\n",
    "        average_weights = sum_scaled_weights(scaled_local_weight_list)\n",
    "    else:\n",
    "        # sort client performances and select top 5 clients\n",
    "        clients_performances.sort(key=lambda x: x[1], reverse=True)\n",
    "        top_5_clients = clients_performances[:5]\n",
    "        for client_info in  top_5_clients:\n",
    "            scaled_weights = scale_model_weights(client_info[3], client_info[2])\n",
    "            scaled_local_weight_list.append(scaled_weights)\n",
    "        average_weights = sum_scaled_weights(scaled_local_weight_list)\n",
    "    global_model.set_weights(average_weights)\n",
    "   \n",
    "    #test global model and print out metrics after each communications round\n",
    "    for(X_test, Y_test) in test_batched:\n",
    "        global_acc, global_loss = test_model(X_test, Y_test, global_model, comm_round)\n",
    "\n",
    "\n",
    "\n",
    "end_time = time.time()\n",
    "training_time = end_time - start_time\n",
    "print(\"Timpul de antrenare:\", training_time, \"secunde\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client_9\n",
      "132/132 [==============================] - 0s 1ms/step\n",
      "comm_round: 0 | local_acc: 1.238% | local_loss: 2.3236281871795654\n",
      "client_10\n",
      "132/132 [==============================] - 0s 1ms/step\n",
      "comm_round: 0 | local_acc: 1.000% | local_loss: 2.3239707946777344\n",
      "client_6\n",
      "132/132 [==============================] - 0s 1ms/step\n",
      "comm_round: 0 | local_acc: 1.548% | local_loss: 2.3200814723968506\n",
      "client_7\n",
      "132/132 [==============================] - 0s 1ms/step\n",
      "comm_round: 0 | local_acc: 85.405% | local_loss: 1.6785995960235596\n",
      "client_1\n",
      "132/132 [==============================] - 0s 1ms/step\n",
      "comm_round: 0 | local_acc: 87.405% | local_loss: 1.6651346683502197\n",
      "client_8\n",
      "132/132 [==============================] - 0s 2ms/step\n",
      "comm_round: 0 | local_acc: 1.452% | local_loss: 2.32011342048645\n",
      "client_3\n",
      "132/132 [==============================] - 0s 2ms/step\n",
      "comm_round: 0 | local_acc: 0.905% | local_loss: 2.3238494396209717\n",
      "client_2\n",
      "132/132 [==============================] - 0s 2ms/step\n",
      "comm_round: 0 | local_acc: 2.976% | local_loss: 2.322370767593384\n",
      "client_4\n",
      "132/132 [==============================] - 0s 1ms/step\n",
      "comm_round: 0 | local_acc: 0.762% | local_loss: 2.3220293521881104\n",
      "client_5\n",
      "132/132 [==============================] - 0s 1ms/step\n",
      "comm_round: 0 | local_acc: 1.548% | local_loss: 2.322998046875\n",
      "132/132 [==============================] - 0s 1ms/step\n",
      "comm_round: 0 | global_acc: 87.786% | global_loss: 1.681438684463501\n",
      "client_9\n",
      "132/132 [==============================] - 0s 1ms/step\n",
      "comm_round: 1 | local_acc: 5.524% | local_loss: 2.304744005203247\n",
      "client_10\n",
      "132/132 [==============================] - 0s 1ms/step\n",
      "comm_round: 1 | local_acc: 7.690% | local_loss: 2.307300090789795\n",
      "client_6\n",
      "132/132 [==============================] - 0s 1ms/step\n",
      "comm_round: 1 | local_acc: 4.833% | local_loss: 2.3049213886260986\n",
      "client_7\n",
      "132/132 [==============================] - 0s 1ms/step\n",
      "comm_round: 1 | local_acc: 87.952% | local_loss: 1.636258602142334\n",
      "client_1\n",
      "132/132 [==============================] - 0s 1ms/step\n",
      "comm_round: 1 | local_acc: 89.952% | local_loss: 1.6333763599395752\n",
      "client_8\n",
      "132/132 [==============================] - 0s 1ms/step\n",
      "comm_round: 1 | local_acc: 5.810% | local_loss: 2.305816650390625\n",
      "client_3\n",
      "132/132 [==============================] - 0s 1ms/step\n",
      "comm_round: 1 | local_acc: 1.976% | local_loss: 2.3068320751190186\n",
      "client_2\n",
      "132/132 [==============================] - 0s 2ms/step\n",
      "comm_round: 1 | local_acc: 3.024% | local_loss: 2.3076534271240234\n",
      "client_4\n",
      "132/132 [==============================] - 0s 1ms/step\n",
      "comm_round: 1 | local_acc: 8.048% | local_loss: 2.306100606918335\n",
      "client_5\n",
      "132/132 [==============================] - 0s 1ms/step\n",
      "comm_round: 1 | local_acc: 10.690% | local_loss: 2.3060495853424072\n",
      "132/132 [==============================] - 0s 1ms/step\n",
      "comm_round: 1 | global_acc: 91.048% | global_loss: 1.7378590106964111\n",
      "client_9\n",
      "132/132 [==============================] - 0s 1ms/step\n",
      "comm_round: 2 | local_acc: 2.333% | local_loss: 2.309295415878296\n",
      "client_10\n",
      "132/132 [==============================] - 0s 1ms/step\n",
      "comm_round: 2 | local_acc: 3.619% | local_loss: 2.3058853149414062\n",
      "client_6\n",
      "132/132 [==============================] - 0s 1ms/step\n",
      "comm_round: 2 | local_acc: 6.167% | local_loss: 2.3064029216766357\n",
      "client_7\n",
      "132/132 [==============================] - 0s 1ms/step\n",
      "comm_round: 2 | local_acc: 90.952% | local_loss: 1.6083810329437256\n",
      "client_1\n",
      "132/132 [==============================] - 0s 1ms/step\n",
      "comm_round: 2 | local_acc: 90.119% | local_loss: 1.615045189857483\n",
      "client_8\n",
      "132/132 [==============================] - 0s 1ms/step\n",
      "comm_round: 2 | local_acc: 8.952% | local_loss: 2.3044614791870117\n",
      "client_3\n",
      "132/132 [==============================] - 0s 2ms/step\n",
      "comm_round: 2 | local_acc: 5.929% | local_loss: 2.3095061779022217\n",
      "client_2\n",
      "  1/132 [..............................] - ETA: 6s"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 34\u001b[0m\n\u001b[0;32m     31\u001b[0m local_model\u001b[38;5;241m.\u001b[39mfit(clients_batched[client], epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m(X_test, Y_test) \u001b[38;5;129;01min\u001b[39;00m test_batched:\n\u001b[1;32m---> 34\u001b[0m     local_acc, local_loss \u001b[38;5;241m=\u001b[39m printLocalPerfm(X_test, Y_test, \n\u001b[0;32m     35\u001b[0m                             local_model, comm_round,clientID)\n\u001b[0;32m     37\u001b[0m local_accuracy_list\u001b[38;5;241m.\u001b[39mappend(local_acc)\n\u001b[0;32m     38\u001b[0m local_weight_list\u001b[38;5;241m.\u001b[39mappend(local_model\u001b[38;5;241m.\u001b[39mget_weights())\n",
      "Cell \u001b[1;32mIn[16], line 33\u001b[0m, in \u001b[0;36mprintLocalPerfm\u001b[1;34m(X_test, y_test, model, comm_round, clientID)\u001b[0m\n\u001b[0;32m     30\u001b[0m local_loss_list \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     32\u001b[0m cce \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlosses\u001b[38;5;241m.\u001b[39mCategoricalCrossentropy(from_logits\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m---> 33\u001b[0m logits \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_test) \n\u001b[0;32m     34\u001b[0m loss \u001b[38;5;241m=\u001b[39m cce(Y_test, logits) \n\u001b[0;32m     35\u001b[0m acc \u001b[38;5;241m=\u001b[39m accuracy_score(tf\u001b[38;5;241m.\u001b[39margmax(logits, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m), tf\u001b[38;5;241m.\u001b[39margmax(Y_test, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py:2629\u001b[0m, in \u001b[0;36mModel.predict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   2627\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _, iterator \u001b[38;5;129;01min\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39menumerate_epochs():  \u001b[38;5;66;03m# Single epoch.\u001b[39;00m\n\u001b[0;32m   2628\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mcatch_stop_iteration():\n\u001b[1;32m-> 2629\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39msteps():\n\u001b[0;32m   2630\u001b[0m             callbacks\u001b[38;5;241m.\u001b[39mon_predict_batch_begin(step)\n\u001b[0;32m   2631\u001b[0m             tmp_batch_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict_function(iterator)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\data_adapter.py:1411\u001b[0m, in \u001b[0;36mDataHandler.steps\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1409\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_insufficient_data:  \u001b[38;5;66;03m# Set by `catch_stop_iteration`.\u001b[39;00m\n\u001b[0;32m   1410\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m-> 1411\u001b[0m original_spe \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_steps_per_execution\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m   1412\u001b[0m can_run_full_execution \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   1413\u001b[0m     original_spe \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1414\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inferred_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1415\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inferred_steps \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_current_step \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m original_spe\n\u001b[0;32m   1416\u001b[0m )\n\u001b[0;32m   1418\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m can_run_full_execution:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:687\u001b[0m, in \u001b[0;36mBaseResourceVariable.numpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    685\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnumpy\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    686\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 687\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread_value()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m    688\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[0;32m    689\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumpy() is only available when eager execution is enabled.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:814\u001b[0m, in \u001b[0;36mBaseResourceVariable.read_value\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    805\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Constructs an op which reads the value of this variable.\u001b[39;00m\n\u001b[0;32m    806\u001b[0m \n\u001b[0;32m    807\u001b[0m \u001b[38;5;124;03mShould be used when there are multiple reads, or when it is desirable to\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    811\u001b[0m \u001b[38;5;124;03m  The value of the variable.\u001b[39;00m\n\u001b[0;32m    812\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    813\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mname_scope(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRead\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 814\u001b[0m   value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_read_variable_op()\n\u001b[0;32m    815\u001b[0m \u001b[38;5;66;03m# Return an identity so it can get placed on whatever device the context\u001b[39;00m\n\u001b[0;32m    816\u001b[0m \u001b[38;5;66;03m# specifies instead of the device where the variable is.\u001b[39;00m\n\u001b[0;32m    817\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m array_ops\u001b[38;5;241m.\u001b[39midentity(value)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:793\u001b[0m, in \u001b[0;36mBaseResourceVariable._read_variable_op\u001b[1;34m(self, no_copy)\u001b[0m\n\u001b[0;32m    791\u001b[0m       result \u001b[38;5;241m=\u001b[39m read_and_set_handle(no_copy)\n\u001b[0;32m    792\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 793\u001b[0m   result \u001b[38;5;241m=\u001b[39m read_and_set_handle(no_copy)\n\u001b[0;32m    795\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m    796\u001b[0m   \u001b[38;5;66;03m# Note that if a control flow context is active the input of the read op\u001b[39;00m\n\u001b[0;32m    797\u001b[0m   \u001b[38;5;66;03m# might not actually be the handle. This line bypasses it.\u001b[39;00m\n\u001b[0;32m    798\u001b[0m   record\u001b[38;5;241m.\u001b[39mrecord_operation(\n\u001b[0;32m    799\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReadVariableOp\u001b[39m\u001b[38;5;124m\"\u001b[39m, [result], [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle],\n\u001b[0;32m    800\u001b[0m       backward_function\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: [x],\n\u001b[0;32m    801\u001b[0m       forward_function\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: [x])\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:783\u001b[0m, in \u001b[0;36mBaseResourceVariable._read_variable_op.<locals>.read_and_set_handle\u001b[1;34m(no_copy)\u001b[0m\n\u001b[0;32m    781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m no_copy \u001b[38;5;129;01mand\u001b[39;00m forward_compat\u001b[38;5;241m.\u001b[39mforward_compatible(\u001b[38;5;241m2022\u001b[39m, \u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m3\u001b[39m):\n\u001b[0;32m    782\u001b[0m   gen_resource_variable_ops\u001b[38;5;241m.\u001b[39mdisable_copy_on_read(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle)\n\u001b[1;32m--> 783\u001b[0m result \u001b[38;5;241m=\u001b[39m gen_resource_variable_ops\u001b[38;5;241m.\u001b[39mread_variable_op(\n\u001b[0;32m    784\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dtype)\n\u001b[0;32m    785\u001b[0m _maybe_set_handle_data(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dtype, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle, result)\n\u001b[0;32m    786\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\ops\\gen_resource_variable_ops.py:589\u001b[0m, in \u001b[0;36mread_variable_op\u001b[1;34m(resource, dtype, name)\u001b[0m\n\u001b[0;32m    587\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tld\u001b[38;5;241m.\u001b[39mis_eager:\n\u001b[0;32m    588\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 589\u001b[0m     _result \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_FastPathExecute(\n\u001b[0;32m    590\u001b[0m       _ctx, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReadVariableOp\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, resource, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, dtype)\n\u001b[0;32m    591\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[0;32m    592\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Metoda 3 \n",
    "#initialize global model\n",
    "start_time = time.time()\n",
    "smlp_global = SimpleMLP()\n",
    "global_model = smlp_global.build(784, 10)\n",
    "#commence global training loop\n",
    "for comm_round in range(comms_round):        \n",
    "    # get the global model's weights\n",
    "    global_weights = global_model.get_weights()\n",
    "    \n",
    "    #initial list to collect local model weights after scalling\n",
    "    scaled_local_weight_list = list()\n",
    "    client_names= list(clients_batched.keys())\n",
    "    random.seed(448)\n",
    "    random.shuffle(client_names)\n",
    "    clientID = 0\n",
    "    local_accuracy_list = []\n",
    "    local_weight_list=[]\n",
    "    \n",
    "    #loop through each client and create new local model\n",
    "    for client in client_names:\n",
    "        print(client)\n",
    "        smlp_local = SimpleMLP()\n",
    "        local_model = smlp_local.build(784, 10)\n",
    "        local_model.compile(loss=loss, \n",
    "                      optimizer=optimizer, \n",
    "                      metrics=metrics)\n",
    "        \n",
    "        #set local model weight to the weight of the global model\n",
    "        local_model.set_weights(global_weights)\n",
    "        local_model.fit(clients_batched[client], epochs=1, verbose=0)\n",
    "\n",
    "        for(X_test, Y_test) in test_batched:\n",
    "            local_acc, local_loss = printLocalPerfm(X_test, Y_test, \n",
    "                                    local_model, comm_round,clientID)\n",
    "        \n",
    "        local_accuracy_list.append(local_acc)\n",
    "        local_weight_list.append(local_model.get_weights())\n",
    "        \n",
    "        #clear session to free memory after each communication round\n",
    "        K.clear_session()\n",
    "        clientID += 1\n",
    "\n",
    "    # calculate sum of local accuracies\n",
    "    sum_local_acc = sum(local_accuracy_list)\n",
    "    # calculate the scaling factors for each client's weights\n",
    "    scaling_factors = [acc / sum_local_acc for acc in local_accuracy_list]\n",
    "    #scale the local weights\n",
    "    scaled_local_weight_list = [[scaling_factors[i] * weight for weight\n",
    "                                 in local_weight_list[i]] for i in \n",
    "                                 range(len(local_weight_list))]\n",
    "    \n",
    "    average_weights = sum_scaled_weights(scaled_local_weight_list)\n",
    "    #update global model \n",
    "    global_model.set_weights(average_weights)\n",
    "   \n",
    "    #test global model and print out metrics after each communications round\n",
    "    for(X_test, Y_test) in test_batched:\n",
    "        global_acc, global_loss = test_model(X_test, Y_test, global_model, comm_round)\n",
    "\n",
    "            \n",
    "end_time = time.time()\n",
    "training_time = end_time - start_time\n",
    "print(\"Timpul de antrenare:\", training_time, \"secunde\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
