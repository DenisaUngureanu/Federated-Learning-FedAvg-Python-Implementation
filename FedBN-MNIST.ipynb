{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99d87cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import random\n",
    "import cv2 \n",
    "import os\n",
    "from imutils import paths\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import accuracy_score\n",
    "import tensorflow as tf   \n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers.legacy import SGD\n",
    "from tensorflow.keras import backend as K\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0726e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(paths, verbose=-1):\n",
    "    '''expects images for each class in seperate dir, \n",
    "    e.g all digits in 0 class in the directory named 0 '''\n",
    "    data = list()\n",
    "    labels = list()\n",
    "    # loop over the input images\n",
    "    for (i, imgpath) in enumerate(paths):\n",
    "        # load the image and extract the class labels\n",
    "        im_gray = cv2.imread(imgpath, cv2.IMREAD_GRAYSCALE) # the image is read as a gray scale\n",
    "        image = np.array(im_gray).flatten() # the image is flattened #Return a copy of the array collapsed into one dimension.\n",
    "        label = imgpath.split(os.path.sep)[-2] # for obtain the class label \n",
    "        # scale the image to [0, 1] and add to list\n",
    "        data.append(image/255) # scale the image to [0, 1]\n",
    "        labels.append(label)\n",
    "        # show an update every `verbose` images\n",
    "        if verbose > 0 and i > 0 and (i + 1) % verbose == 0:\n",
    "            print(\"[INFO] processed {}/{}\".format(i + 1, len(paths)))\n",
    "    # return a tuple of the data and labels\n",
    "    return data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00066817",
   "metadata": {},
   "outputs": [],
   "source": [
    "#declare path to your mnist data folder\n",
    "img_path = ' '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "35161da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the path list using the path object\n",
    "image_paths = list(paths.list_images(img_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "91151028",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] processed 10000/42000\n",
      "[INFO] processed 20000/42000\n",
      "[INFO] processed 30000/42000\n",
      "[INFO] processed 40000/42000\n"
     ]
    }
   ],
   "source": [
    "#apply our function\n",
    "image_list, label_list = load(image_paths, verbose=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5bebc834",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converteste etichetele claselor Ã®n format binar.\n",
    "lb = LabelBinarizer()\n",
    "label_list = lb.fit_transform(label_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cd3ae847",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split data into training and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(image_list, \n",
    "                                                    label_list, \n",
    "                                                    test_size=0.1, \n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260ba38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_clientsIID(image_list, label_list, num_clients=1, initial='clients'):\n",
    "    '''\n",
    "        args: \n",
    "            image_list: a list of numpy arrays of training images\n",
    "            label_list:a list of binarized labels for each image\n",
    "            num_client: number of fedrated members (clients)\n",
    "            initials: the clients'name prefix, e.g, clients_1 \n",
    "            \n",
    "    '''\n",
    "    #create a list of client names\n",
    "    client_names = ['{}_{}'.format(initial, i+1) for i in range(num_clients)]\n",
    "\n",
    "    #randomize the data\n",
    "    data = list(zip(image_list, label_list))\n",
    "    random.seed(10)\n",
    "    random.shuffle(data)\n",
    "\n",
    "    #shard data and place at each client\n",
    "    size = len(data)//num_clients\n",
    "    shards = [data[i:i + size] for i in range(0, size*num_clients, size)]\n",
    "    \n",
    "    #number of clients must equal number of shards\n",
    "    assert(len(shards) == len(client_names))\n",
    "\n",
    "    return {client_names[i] : shards[i] for i in range(len(client_names))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5d51ae69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_clients_nonIID(image_list, label_list, num_clients=1, initial='clients'):\n",
    "    client_names = ['{}_{}'.format(initial, i+1) for i in range(num_clients)]\n",
    "\n",
    "    shards = {}\n",
    "    for i, client_name in enumerate(client_names):\n",
    "        class_index = i % len(label_list[0])  \n",
    "        class_images = [image for image, label in zip(image_list, label_list) if label[class_index] == 1] \n",
    "        class_labels = [label for label in label_list if label[class_index] == 1]\n",
    "        shards[client_name] = list(zip(class_images, class_labels))\n",
    "\n",
    "    return shards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a524c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_clients_diff_percentages(image_list, label_list, num_clients=1, initial='clients'):\n",
    "    '''\n",
    "        args: \n",
    "            image_list: a list of numpy arrays of training images\n",
    "            label_list:a list of binarized labels for each image\n",
    "            num_client: number of fedrated members (clients)\n",
    "            initials: the clients'name prefix, e.g, clients_1 \n",
    "            \n",
    "    '''\n",
    "\n",
    "    #create a list of client names\n",
    "    client_names = ['{}_{}'.format(initial, i+1) for i in range(num_clients)]\n",
    "    client_weights = [0.05, 0.10, 0.15, 0.10, 0.05,0.15,0.10,0.05,0.15,0.10]\n",
    "    assert sum(client_weights) == 1\n",
    "\n",
    "    data = list(zip(image_list, label_list))\n",
    "    random.seed(10)\n",
    "    random.shuffle(data)\n",
    "    \n",
    "    start = 0\n",
    "    shards = []\n",
    "    for i in range(num_clients):\n",
    "        end = start + int(len(data) * (client_weights[i]))\n",
    "        client_data = data[start:end]\n",
    "        start = end\n",
    "        shards.append(client_data)\n",
    "        print(f\"Number of data points: {len(client_data)}\")\n",
    "    \n",
    "    #number of clients must equal number of shards\n",
    "    assert(len(shards) == len(client_names))\n",
    "\n",
    "    return {client_names[i] : shards[i] for i in range(len(client_names))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f0184afe",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'create_clientsnonIID' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m clients\u001b[38;5;241m=\u001b[39m create_clientsnonIID(X_train, y_train, num_clients\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, initial\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclient\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'create_clientsnonIID' is not defined"
     ]
    }
   ],
   "source": [
    "clients= create_clientsnonIID(X_train, y_train, num_clients=10, initial='client')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e348e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_data(data_shard, bs=32):\n",
    "    '''Takes in a clients data shard and create a tfds object off it\n",
    "    args:\n",
    "        shard: a data, label constituting a client's data shard\n",
    "        bs:batch size\n",
    "    return:\n",
    "        tfds object'''\n",
    "    #seperate shard into data and labels lists\n",
    "    data, label = zip(*data_shard)\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((list(data), list(label)))\n",
    "    return dataset.shuffle(len(label)).batch(bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3385bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#process and batch the training data for each client\n",
    "clients_batched = dict()\n",
    "for (client_name, data) in clients.items():\n",
    "    clients_batched[client_name] = batch_data(data)\n",
    "\n",
    "#process and batch the test set  \n",
    "test_batched = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1760b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP_BN:\n",
    "    @staticmethod\n",
    "    def build(shape, classes):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(256, input_shape=(shape,)))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(Dense(128))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(Dense(classes))\n",
    "        model.add(Activation(\"softmax\"))\n",
    "        #model.summary()\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6e74cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP2:\n",
    "    @staticmethod\n",
    "    def build(shape, classes):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(256, input_shape=(shape,)))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(Dense(128))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(Dense(classes))\n",
    "        model.add(Activation(\"softmax\"))\n",
    "        #model.summary()\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8ca610",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "comms_round = 100\n",
    "\n",
    "loss='categorical_crossentropy'\n",
    "metrics = ['accuracy']\n",
    "\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=0.01,\n",
    "    decay_steps=10000,\n",
    "    decay_rate=0.9) \n",
    "\n",
    "# Create the optimizer with the learning rate schedule\n",
    "optimizer = SGD(learning_rate=lr_schedule, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40502ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def weight_scalling_factor(clients_trn_data, client_name):\n",
    "    client_names = list(clients_trn_data.keys())\n",
    "    #get the bs\n",
    "    bs = list(clients_trn_data[client_name])[0][0].shape[0]\n",
    "    #first calculate the total training data points across clinets\n",
    "    global_count = sum([tf.data.experimental.cardinality(clients_trn_data[client_name]).numpy() for client_name in client_names])*bs\n",
    "    # get the total number of data points held by a client\n",
    "    local_count = tf.data.experimental.cardinality(clients_trn_data[client_name]).numpy()*bs\n",
    "    return local_count/global_count\n",
    "\n",
    "def scale_model_weights(weight, scalar):\n",
    "    '''function for scaling a models weights'''\n",
    "    weight_final = []\n",
    "    steps = len(weight)\n",
    "    for i in range(steps):\n",
    "        weight_final.append(scalar * weight[i])\n",
    "    return weight_final\n",
    "\n",
    "\n",
    "def sum_scaled_weights(scaled_weight_list):\n",
    "    '''Return the sum of the listed scaled weights. The is equivalent to scaled avg of the weights'''\n",
    "    avg_grad = list()\n",
    "    #get the average grad accross all client gradients\n",
    "    for grad_list_tuple in zip(*scaled_weight_list):\n",
    "        layer_mean = tf.math.reduce_sum(grad_list_tuple, axis=0)\n",
    "        avg_grad.append(layer_mean)\n",
    "\n",
    "    return avg_grad\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24bf47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(X_test, Y_test,  model, comm_round):\n",
    "    global performanceExport\n",
    "    \n",
    "    cce = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "    logits = model.predict(X_test)\n",
    "    loss = cce(Y_test, logits)\n",
    "    acc = accuracy_score(tf.argmax(logits, axis=1), tf.argmax(Y_test, axis=1))\n",
    "    \n",
    "    new_performanceExport = pd.DataFrame({'Model':'GLOBAL', 'Com_Round': int(comm_round), 'accuracy': float(acc * 100), 'loss': float(loss)}, index=[0])\n",
    "    performanceExport = pd.concat([performanceExport, new_performanceExport])\n",
    "    \n",
    "    path = f' '\n",
    "    performanceExport.to_excel (path, index = False, header=True)\n",
    "   \n",
    "    print('comm_round: {} | global_acc: {:.3%} | global_loss: {}'.format(comm_round, acc, loss))\n",
    "    return acc, loss\n",
    "\n",
    "#print the performances from each client using test data\n",
    "def printLocalPerfm(X_test, y_test, model, comm_round, clientID):\n",
    "    global performanceExport\n",
    "    global accuracy\n",
    "    \n",
    "    #list for local accuracy and loss (for each model from client)\n",
    "    global local_acc_list \n",
    "    local_acc_list = []\n",
    "    global local_loss_list\n",
    "    local_loss_list = []\n",
    "    \n",
    "    cce = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "    logits = model.predict(X_test) \n",
    "    loss = cce(Y_test, logits) \n",
    "    acc = accuracy_score(tf.argmax(logits, axis=1), tf.argmax(Y_test, axis=1))\n",
    "    local_acc_list.append(loss)\n",
    "    local_loss_list.append(acc)\n",
    "\n",
    "    new_performanceExport = pd.DataFrame({'Model': f'model_{int(clientID)}', 'Com_Round': int(comm_round), 'accuracy': float(acc * 100), 'loss': float(loss)}, index=[0])\n",
    "    performanceExport = pd.concat([performanceExport, new_performanceExport])\n",
    "   \n",
    "    path = f' '\n",
    "    performanceExport.to_excel (path, index = False, header=True) \n",
    "    \n",
    "    print('comm_round: {} | local_acc: {:.3%} | local_loss: {}'.format(comm_round, acc, loss))\n",
    "    return acc, loss\n",
    "\n",
    "\n",
    "performanceExport = pd.DataFrame(columns = ['Model', 'Com_Round', 'accuracy', 'loss'])\n",
    "accuracy = pd.DataFrame(columns = ['Model', 'Com_Round', 'accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b62066",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#initialize global model\n",
    "smlp_global = SimpleMLP_BN()\n",
    "new_smlp_global = SimpleMLP2()\n",
    "\n",
    "global_model = smlp_global.build(784, 10)\n",
    "new_global_model=new_smlp_global.build(784,10)\n",
    "\n",
    "\n",
    "#commence global training loop\n",
    "for comm_round in range(comms_round):        \n",
    "    # get the global model's weights \n",
    "    global_weights = global_model.get_weights()\n",
    "    scaled_local_weight_list = list()\n",
    "    client_names= list(clients_batched.keys())\n",
    "    random.seed(448)\n",
    "    random.shuffle(client_names)\n",
    "    clientID = 0\n",
    "    \n",
    "    #loop through each client and create new local model\n",
    "    for client in client_names:\n",
    "        smlp_local = SimpleMLP_BN()\n",
    "        local_model = smlp_local.build(784, 10)\n",
    "        local_model.compile(loss=loss, \n",
    "                      optimizer=optimizer, \n",
    "                      metrics=metrics)\n",
    "        \n",
    "        #initialize local models only for layers that are not BN\n",
    "        for l, layer in enumerate(global_model.layers):\n",
    "            if not isinstance(layer, BatchNormalization):    \n",
    "                local_model.layers[l].set_weights(layer.get_weights())\n",
    "\n",
    "        local_model.fit(clients_batched[client], epochs=1, verbose=0)\n",
    "       \n",
    "        new_smlp = SimpleMLP2()\n",
    "        new_local_model=new_smlp.build(784,10)\n",
    "        #transfer weights from local model to new_local_model\n",
    "        j=0\n",
    "        for i, layer in enumerate(local_model.layers):\n",
    "            if not isinstance(layer, tf.keras.layers.BatchNormalization) :\n",
    "                new_local_model.layers[j].set_weights(layer.get_weights())\n",
    "                j=j+1\n",
    "                \n",
    "        scaling_factor = weight_scalling_factor(clients_batched, client)\n",
    "        scaled_weights = scale_model_weights(new_local_model.get_weights(), scaling_factor)\n",
    "        scaled_local_weight_list.append(scaled_weights)\n",
    "        \n",
    "        for(X_test, Y_test) in test_batched:\n",
    "            global_acc, global_loss = printLocalPerfm(X_test, Y_test,local_model, \n",
    "                                     comm_round, clientID)\n",
    "        \n",
    "        #clear session to free memory after each communication round\n",
    "        K.clear_session()\n",
    "        clientID += 1\n",
    "        \n",
    "    #get the average over all the local model\n",
    "    average_weights = sum_scaled_weights(scaled_local_weight_list)\n",
    "    \n",
    "    #set the weights of a new global model that does not contain BN layer\n",
    "    new_global_model.set_weights(average_weights)\n",
    "    \n",
    "    #update the original global model\n",
    "    i=0\n",
    "    for l, layer in enumerate(global_model.layers):\n",
    "        if not isinstance(layer, BatchNormalization):\n",
    "            global_model.layers[l].set_weights(new_global_model.layers[i].get_weights())\n",
    "            i=i+1\n",
    "   \n",
    "\n",
    "    #test global model and print out metrics after each communications round\n",
    "    for(X_test, Y_test) in test_batched:\n",
    "        global_acc, global_loss = test_model(X_test, Y_test, global_model, comm_round)\n",
    "\n",
    "\n",
    "            \n",
    "              \n",
    "         "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
